{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2 -- \"Читаем хабр, используем Наташу, изучаем именованные сущности)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.\n",
    "\n",
    "Открываем википедию *(за что мне это)*, выбираем статью не менее 3000 слов, определяем именованные сущности, которые фигурируют в тексте, определяем, какие границы этих сущностей хотим задать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст большое, поэтому... Фух...\n",
    "\n",
    "В качестве сущностей можем выделить: \n",
    "- локации -- Европа, Земля, Новая Зеландия\n",
    "- временные периоды -- XIX век\n",
    "- организации -- ВОЗ\n",
    "- научные (и псевдонаучные, привет, википедия) определения -- суицид, парасуицид, псевдосуицид, скрытый суицид, попытка самоубийства, антисуицидальные факторы личности\n",
    "- причины суицида *(сомнительно, конечно, что это можно прям выделять)* -- измены, семейные конфликты, неудачная любовь, госпитализм\n",
    "- болезни -- ВИЧ-инфекция, нефрологические болезни, СПИД, бронхиальная астма, язвенная болезнь\n",
    "- персоны -- А.Г. Амбрумова\n",
    "- уровень и риск суицида -- как по странам, так и по местам, по полу и прочему, границу очертить тяжело\n",
    "- способы суицида -- повешение, удушение, утопление\n",
    "- наименование религиозных направлений -- католицизм, буддизм, джайнизм синтоизм + описание того, как в вере относятся к самоубийствам\n",
    "\n",
    "На самом деле -- это не все. Мне кажется, здесь можно просто каждое слово вынести как именованную сущность и по сути прийти к морфологическому анализу :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.\n",
    "\n",
    "Извлекаем как можно больше именованных сущностей готовыми правилами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Хорошо, что так можно сделать\n",
    "# Жаль, что мне это не понравилось\n",
    "\n",
    "response = requests.get(\n",
    "    'https://ru.wikipedia.org/w/api.php',\n",
    "    params={\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "        'titles': 'Самоубийство',\n",
    "    }\n",
    ").json()\n",
    "\n",
    "f = open('analyze.txt', 'w')\n",
    "f.write(next(iter(response['query']['pages'].values()))['extract'])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ┌► Чрезвычайно  advmod\n",
      "        ┌►└─ сложный      amod\n",
      "        │ ┌► этический    amod\n",
      "        └─└─ вопрос       nsubj\n",
      "┌─┌─────└─┌─ представляет \n",
      "│ │     ┌─└► отнесение    obj\n",
      "│ │     │ ┌► к            case\n",
      "│ │ ┌───└►└─ самоубийству nmod\n",
      "│ │ │ ┌────► (            punct\n",
      "│ │ │ │ ┌──► либо         cc\n",
      "│ │ │ │ │ ┌► к            case\n",
      "│ │ └►└─└─└─ убийству     parataxis\n",
      "│ │     └──► )            punct\n",
      "│ └────────► эвтаназии    obl\n",
      "└──────────► .            punct\n"
     ]
    }
   ],
   "source": [
    "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, PER, NamesExtractor, Doc \n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "text = open('analyze.txt').read()\n",
    "\n",
    "doc = Doc(text)\n",
    "\n",
    "# Рубрика \"угадай, какую последовательность вызовов надо использовать, чтобы все это заработало\"\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "for it in doc.tokens:\n",
    "    it.lemmatize(morph_vocab)\n",
    "\n",
    "doc.parse_syntax(syntax_parser)\n",
    "doc.sents[1].syntax.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tag_ner(ner_tagger)\n",
    "doc.ner.print()\n",
    "# аккуратно, тут будет много буков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSpan(stop=13, type='PER', text='Самоуби́йство', tokens=[...], normal='Самоуби́йство'),\n",
       " DocSpan(start=471, stop=477, type='LOC', text='Европы', tokens=[...], normal='Европа'),\n",
       " DocSpan(start=583, stop=626, type='ORG', text='Всемирной организации здравоохранения (ВОЗ)', tokens=[...], normal='Всемирная организация здравоохранения (ВОЗ)'),\n",
       " DocSpan(start=663, stop=668, type='LOC', text='Земли', tokens=[...], normal='Земля'),\n",
       " DocSpan(start=746, stop=749, type='ORG', text='ВОЗ', tokens=[...], normal='ВОЗ'),\n",
       " DocSpan(start=1030, stop=1047, type='LOC', text='Латинской Америке', tokens=[...], normal='Латинская Америка'),\n",
       " DocSpan(start=1119, stop=1130, type='LOC', text='Центральной', tokens=[...], normal='Центральная'),\n",
       " DocSpan(start=1133, stop=1148, type='LOC', text='Северной Европе', tokens=[...], normal='Северная Европа'),\n",
       " DocSpan(start=1150, stop=1166, type='LOC', text='Северной Америке', tokens=[...], normal='Северная Америка'),\n",
       " DocSpan(start=1168, stop=1186, type='LOC', text='Юго-Восточной Азии', tokens=[...], normal='Юго-Восточная Азия'),\n",
       " DocSpan(start=1189, stop=1203, type='LOC', text='Западной части', tokens=[...], normal='Западная часть'),\n",
       " DocSpan(start=1204, stop=1217, type='LOC', text='Тихого океана', tokens=[...], normal='Тихий океан'),\n",
       " DocSpan(start=1219, stop=1228, type='LOC', text='Австралия', tokens=[...], normal='Австралия'),\n",
       " DocSpan(start=1230, stop=1236, type='LOC', text='Канада', tokens=[...], normal='Канада'),\n",
       " DocSpan(start=1238, stop=1243, type='LOC', text='Индия', tokens=[...], normal='Индия'),\n",
       " DocSpan(start=1245, stop=1259, type='LOC', text='Новая Зеландия', tokens=[...], normal='Новая Зеландия'),\n",
       " DocSpan(start=1261, stop=1264, type='LOC', text='США', tokens=[...], normal='США'),\n",
       " DocSpan(start=1323, stop=1328, type='LOC', text='Литве', tokens=[...], normal='Литва'),\n",
       " DocSpan(start=1330, stop=1340, type='LOC', text='Белоруссии', tokens=[...], normal='Белоруссия'),\n",
       " DocSpan(start=1342, stop=1348, type='LOC', text='России', tokens=[...], normal='Россия'),\n",
       " DocSpan(start=1350, stop=1359, type='LOC', text='Шри-Ланке', tokens=[...], normal='Шри-Ланке'),\n",
       " DocSpan(start=1361, stop=1371, type='LOC', text='Казахстане', tokens=[...], normal='Казахстан'),\n",
       " DocSpan(start=1373, stop=1380, type='LOC', text='Венгрии', tokens=[...], normal='Венгрия'),\n",
       " DocSpan(start=1382, stop=1388, type='LOC', text='Японии', tokens=[...], normal='Япония'),\n",
       " DocSpan(start=1393, stop=1400, type='LOC', text='Украине', tokens=[...], normal='Украина'),\n",
       " DocSpan(start=1405, stop=1411, type='LOC', text='Латвии', tokens=[...], normal='Латвия'),\n",
       " DocSpan(start=5445, stop=5448, type='ORG', text='ВОЗ', tokens=[...], normal='ВОЗ'),\n",
       " DocSpan(start=6092, stop=6098, type='PER', text='Сенека', tokens=[...], normal='Сенека'),\n",
       " DocSpan(start=6100, stop=6107, type='PER', text='Роммель', tokens=[...], normal='Роммель'),\n",
       " DocSpan(start=13064, stop=13080, type='PER', text='А. Г. Амбрумовой', tokens=[...], normal='А. Г. Амбрумовой'),\n",
       " DocSpan(start=13368, stop=13379, type='PER', text='Аффективные', tokens=[...], normal='Аффективные'),\n",
       " DocSpan(start=15415, stop=15424, type='PER', text='Аарон Бек', tokens=[...], normal='Аарон Бек'),\n",
       " DocSpan(start=16587, stop=16619, type='ORG', text='Совета по вопросам наркополитики', tokens=[...], normal='Совет по вопросам наркополитики'),\n",
       " DocSpan(start=16634, stop=16648, type='LOC', text='Великобритании', tokens=[...], normal='Великобритания'),\n",
       " DocSpan(start=16649, stop=16659, type='PER', text='Дэвид Натт', tokens=[...], normal='Дэвид Натт'),\n",
       " DocSpan(start=16886, stop=16889, type='ORG', text='ВОЗ', tokens=[...], normal='ВОЗ'),\n",
       " DocSpan(start=17809, stop=17815, type='LOC', text='России', tokens=[...], normal='Россия'),\n",
       " DocSpan(start=18389, stop=18395, type='LOC', text='России', tokens=[...], normal='Россия'),\n",
       " DocSpan(start=18515, stop=18531, type='PER', text='М. Г. Дебольский', tokens=[...], normal='М. Г. Дебольский'),\n",
       " DocSpan(start=18534, stop=18548, type='PER', text='И. А. Матвеева', tokens=[...], normal='И. А. Матвеев'),\n",
       " DocSpan(start=18946, stop=18952, type='LOC', text='России', tokens=[...], normal='Россия'),\n",
       " DocSpan(start=19214, stop=19220, type='LOC', text='России', tokens=[...], normal='Россия'),\n",
       " DocSpan(start=19293, stop=19297, type='ORG', text='СИЗО', tokens=[...], normal='СИЗО'),\n",
       " DocSpan(start=19318, stop=19322, type='ORG', text='СИЗО', tokens=[...], normal='СИЗО'),\n",
       " DocSpan(start=19382, stop=19386, type='ORG', text='СИЗО', tokens=[...], normal='СИЗО'),\n",
       " DocSpan(start=19568, stop=19572, type='ORG', text='СИЗО', tokens=[...], normal='СИЗО'),\n",
       " DocSpan(start=19660, stop=19664, type='ORG', text='СИЗО', tokens=[...], normal='СИЗО'),\n",
       " DocSpan(start=22654, stop=22660, type='LOC', text='России', tokens=[...], normal='Россия'),\n",
       " DocSpan(start=22679, stop=22686, type='LOC', text='Украине', tokens=[...], normal='Украина'),\n",
       " DocSpan(start=25608, stop=25611, type='LOC', text='США', tokens=[...], normal='США')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "\n",
    "# for span in doc.spans:\n",
    "#     if span.type == PER:\n",
    "#         print(span)\n",
    "\n",
    "doc.spans[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Легко увидеть, что...\n",
    "В общем и целом мы очень хорошо справляемся с местами локаций. Шутка про то, что самоубийство -- это имя, вообще разрыв. Также прикольно, что у нас \"Аффективные\" -- это тоже персона, а СИЗО -- это организация... Но происходит это по понятным причинам, а, значит, можно предугадать такое поведение и просто хорошенько предобработать текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Правило 1\n",
    "Далее первое правило нацелено на то, чтобы выделить в тексте какие-то заболевания, сопутствующие теме суицидологии. Я рассчитываю на то, что это позволит получить список \"симптоматик\" для более строгого надзора за людьми в группах риска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['серьёзных', 'заболеваниях']\n",
      "['душевную', 'боль']\n",
      "['болезнь', 'близкого']\n",
      "['соматическое', 'заболевание']\n",
      "['психическими', 'травмами']\n",
      "['хроническими', 'болями']\n",
      "['хронические', 'заболевания']\n",
      "['Соматические', 'болезни']\n",
      "['соматическими', 'болезнями']\n",
      "['хроническими', 'заболеваниями']\n",
      "['и', 'травмы']\n",
      "['онкологические', 'заболевания']\n",
      "['непереносимые', 'боли']\n",
      "['нефрологические', 'болезни']\n",
      "['инфекцией', 'повышенный']\n",
      "['соматических', 'болезней']\n",
      "['хронические', 'заболевания']\n",
      "['такие', 'заболевания']\n",
      "['язвенная', 'болезнь']\n",
      "['соматических', 'заболеваниях']\n",
      "['психических', 'заболеваний']\n",
      "['опасными', 'заболеваниями']\n",
      "['душевной', 'боли']\n",
      "['неизлечимые', 'болезни']\n",
      "['серьёзного', 'заболевания']\n",
      "['лечения', 'заболевания']\n",
      "['неизлечимому', 'заболеванию']\n",
      "['его', 'болезни']\n"
     ]
    }
   ],
   "source": [
    "from yargy import Parser, rule, and_, or_\n",
    "from yargy.predicates import gram, is_capitalized, dictionary\n",
    "from yargy.pipelines import morph_pipeline\n",
    "\n",
    "# Берем слова, которые должны описывать какой-то disease\n",
    "DESIESE = morph_pipeline([\n",
    "    'инфекция',\n",
    "    'болезнь',\n",
    "    'боль',\n",
    "    'заболевание',\n",
    "    'травма'\n",
    "])\n",
    "\n",
    "# Наименование болезни, инфекции, болей, которое скорее всего будет либо существительным, либо прилагательным (полным)\n",
    "DESIESE_NAME = or_(\n",
    "    gram(\"NOUN\"),\n",
    "    gram(\"ADJF\"),\n",
    "    # gram(\"ADJS\")\n",
    ")\n",
    "\n",
    "# Предполагаем, что у нас либо сначала наименование, а потом что-то ужасное, либо наоборот\n",
    "RULE = or_(\n",
    "    rule(DESIESE, DESIESE_NAME),\n",
    "    rule(DESIESE_NAME, DESIESE),\n",
    ")\n",
    "\n",
    "parser = Parser(RULE)\n",
    "for match in parser.findall(text):\n",
    "    print([_.value for _ in match.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат:\n",
    "\n",
    "Получилось не очень хорошо, но и не очень плохо. Мы не смогли найти ВИЧ-инфекцию и бронхиальную астму, однако нашли большой перечень хороших \"звоночков\". Но также имеются и проблемы: нашли сочетания по типу \"его болезни\" и \"и травмы\". С одной стороны, это ок, а с другой -- до полной автоматизации тут далеко."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Правило 2\n",
    "Сделаем вид, что нам очень лень самим читать текст и мы хотим узнать, кто что сделал во сколько раз чаще, чтобы привести эти в дипломчике. Такие сравнения, благо, часто носят одну и ту же форму, поэтому мы без проблем можем это попробовать сделать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yargy import Parser, rule, or_\n",
    "from yargy.predicates import gram, is_capitalized, dictionary\n",
    "\n",
    "# в\n",
    "PREP = gram(\"PREP\")\n",
    "\n",
    "# 4\n",
    "NUMB = or_(\n",
    "    gram(\"NUMB\"),\n",
    "    gram(\"NUMR\")\n",
    ")\n",
    "\n",
    "# раза\n",
    "TIMES = morph_pipeline([\n",
    "    'раза',\n",
    "    'раз'\n",
    "])\n",
    "\n",
    "# чаще, реже\n",
    "ADJ = or_(\n",
    "    gram(\"ADJF\"),\n",
    "    gram(\"ADJS\"),\n",
    "    gram(\"COMP\")\n",
    ")\n",
    "\n",
    "# умирают, убивают, режут, кушают\n",
    "VERB = gram(\"VERB\")\n",
    "\n",
    "RULE = or_(\n",
    "    rule(PREP, NUMB, TIMES, ADJ, VERB), # \"в 10 раз чаще умирают\"\n",
    "    rule(PREP, NUMB, TIMES, ADJ, gram(\"NOUN\")), # в 10 раз чаще мужчин\n",
    "    rule(PREP, NUMB, TIMES), # \"в 10 раз\"\n",
    ")\n",
    "\n",
    "parser = Parser(RULE)\n",
    "for match in parser.findall(\"Мужчины совершают самоубийство в 4.34 раза чаще, чем женщины (хотя женщины совершают в 4 раза больше попыток самоубийства)\"):\n",
    "    print([_.value for _ in match.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И вот это уже незадача. Почему-то именно gram(\"NUMB\") никак не может распознать число 4 или 4.34. Почему? Для меня загадка и интернет на этот счет молчит.\n",
    "\n",
    "У pymorphy есть еще is_roman_number, но это нам как бы не подойдет... Очень странная и плохая дичь ._.\n",
    "\n",
    "P.S. -- ROMN тоже не работает ;-;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Правило 3\n",
    "\n",
    "Ну что ж, после той неудачи, давайте попробуем найти что-нибудь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', 'тысячи', 'человек']\n",
      "['1', 'миллиона', 'человек']\n",
      "['110', 'Уголовного', 'кодекса']\n",
      "['120', '-', 'ФЗ']\n",
      "['№', '139', '-', 'ФЗ']\n",
      "['124', 'УК', 'РФ']\n"
     ]
    }
   ],
   "source": [
    "from yargy import Parser, rule, or_, not_\n",
    "from yargy.predicates import gram, is_capitalized, dictionary, lte, gte, is_single\n",
    "\n",
    "\n",
    "# Номер\n",
    "NUMB_FIR = morph_pipeline([\n",
    "    \"номер\",\n",
    "    \"#\",\n",
    "    \"№\"\n",
    "])\n",
    "\n",
    "# Числительное (а вот так работает))))\n",
    "NUMB_SEC = and_(\n",
    "    gte(1),\n",
    "    lte(999)\n",
    ")\n",
    "\n",
    "# УК, РФ, ФЗ и прочее\n",
    "LAW = and_(\n",
    "    gram(\"NOUN\"),\n",
    "    is_single(),\n",
    "    not_(gram(\"PREP\")), # ну да, предлоги же -- это существительные...\n",
    "    not_(gram(\"COMP\")),\n",
    "    not_(gram(\"CONJ\")),\n",
    "    not_(gram(\"NUMR\")),\n",
    "    or_(\n",
    "        gram(\"nomn\"), # именительный\n",
    "        gram(\"accs\"), # винительный\n",
    "        gram(\"gent\") # родительный\n",
    "    )\n",
    ")\n",
    "\n",
    "RULE = or_(\n",
    "    rule(NUMB_FIR, NUMB_SEC, \"-\", LAW), # № 100-ФЗ\n",
    "    rule(NUMB_FIR, NUMB_SEC, LAW), # № 120 УК\n",
    "    rule(NUMB_SEC, \"-\", LAW), # 120-ФЗ\n",
    "    rule(NUMB_SEC, LAW, LAW), # 120 УК РФ, 120 ГК РФ\n",
    ")\n",
    "\n",
    "parser = Parser(RULE)\n",
    "for match in parser.findall(text):\n",
    "    print([_.value for _ in match.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат:\n",
    "было бы проще и лучше воспользоваться регуляркой со словарем \"УК\", \"ФЗ\", \"ГК\", однако так как тут природа текста не очень профессиональная, она позволила также определить и \"Уголовного кодекса\", что круто, даже несмотря на то, что мне так и не удалось отрезать эти несчастные \"3 тысячи человек\" и \"1 миллиона человек\"\n",
    "\n",
    "А еще забавно, что у нас \"тысячи\" -- это слово в единственном числе."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
