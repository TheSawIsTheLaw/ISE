{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2 -- \"Читаем хабр, используем Наташу, изучаем именованные сущности)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.\n",
    "\n",
    "Открываем википедию *(за что мне это)*, выбираем статью не менее 3000 слов, определяем именованные сущности, которые фигурируют в тексте, определяем, какие границы этих сущностей хотим задать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст большое, поэтому... Фух...\n",
    "\n",
    "В качестве сущностей можем выделить: \n",
    "- локации -- Европа, Земля, Новая Зеландия\n",
    "- временные периоды -- XIX век\n",
    "- организации -- ВОЗ\n",
    "- научные (и псевдонаучные, привет, википедия) определения -- суицид, парасуицид, псевдосуицид, скрытый суицид, попытка самоубийства, антисуицидальные факторы личности\n",
    "- причины суицида *(сомнительно, конечно, что это можно прям выделять)* -- измены, семейные конфликты, неудачная любовь, госпитализм\n",
    "- болезни -- ВИЧ-инфекция, нефрологические болезни, СПИД, бронхиальная астма, язвенная болезнь\n",
    "- персоны -- А.Г. Амбрумова\n",
    "- уровень и риск суицида -- как по странам, так и по местам, по полу и прочему, границу очертить тяжело\n",
    "- способы суицида -- повешение, удушение, утопление\n",
    "- наименование религиозных направлений -- католицизм, буддизм, джайнизм синтоизм + описание того, как в вере относятся к самоубийствам\n",
    "\n",
    "На самом деле -- это не все. Мне кажется, здесь можно просто каждое слово вынести как именованную сущность и по сути придти к морфологическому анализу :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.\n",
    "\n",
    "Извлекаем как можно больше именованных сущностей готовыми правилами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Хорошо, что так можно сделать\n",
    "# Жаль, что мне это не понравилось\n",
    "\n",
    "response = requests.get(\n",
    "    'https://ru.wikipedia.org/w/api.php',\n",
    "    params={\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "        'titles': 'Самоубийство',\n",
    "    }\n",
    ").json()\n",
    "\n",
    "f = open('analyze.txt', 'w')\n",
    "f.write(next(iter(response['query']['pages'].values()))['extract'])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ┌► Чрезвычайно  advmod\n",
      "        ┌►└─ сложный      amod\n",
      "        │ ┌► этический    amod\n",
      "        └─└─ вопрос       nsubj\n",
      "┌─┌─────└─┌─ представляет \n",
      "│ │     ┌─└► отнесение    obj\n",
      "│ │     │ ┌► к            case\n",
      "│ │ ┌───└►└─ самоубийству nmod\n",
      "│ │ │ ┌────► (            punct\n",
      "│ │ │ │ ┌──► либо         cc\n",
      "│ │ │ │ │ ┌► к            case\n",
      "│ │ └►└─└─└─ убийству     parataxis\n",
      "│ │     └──► )            punct\n",
      "│ └────────► эвтаназии    obl\n",
      "└──────────► .            punct\n"
     ]
    }
   ],
   "source": [
    "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, PER, NamesExtractor, Doc \n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "text = open('analyze.txt').read()\n",
    "\n",
    "doc = Doc(text)\n",
    "\n",
    "# Рубрика \"угадай, какую последовательность вызовов надо использовать, чтобы все это заработало\"\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "for it in doc.tokens:\n",
    "    it.lemmatize(morph_vocab)\n",
    "\n",
    "doc.parse_syntax(syntax_parser)\n",
    "doc.sents[1].syntax.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tag_ner(ner_tagger)\n",
    "doc.ner.print()\n",
    "# аккуратно, тут будет много буков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocSpan(stop=13, type='PER', text='Самоуби́йство', tokens=[...], normal='Самоуби́йство')\n",
      "DocSpan(start=6092, stop=6098, type='PER', text='Сенека', tokens=[...], normal='Сенека', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=6100, stop=6107, type='PER', text='Роммель', tokens=[...], normal='Роммель', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=13064, stop=13080, type='PER', text='А. Г. Амбрумовой', tokens=[...], normal='А. Г. Амбрумовой', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=13368, stop=13379, type='PER', text='Аффективные', tokens=[...], normal='Аффективные')\n",
      "DocSpan(start=15415, stop=15424, type='PER', text='Аарон Бек', tokens=[...], normal='Аарон Бек', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=16649, stop=16659, type='PER', text='Дэвид Натт', tokens=[...], normal='Дэвид Натт', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=18515, stop=18531, type='PER', text='М. Г. Дебольский', tokens=[...], normal='М. Г. Дебольский', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=18534, stop=18548, type='PER', text='И. А. Матвеева', tokens=[...], normal='И. А. Матвеев', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=27861, stop=27875, type='PER', text='Л. З. Трегубов', tokens=[...], normal='Л. З. Трегубов', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=27878, stop=27889, type='PER', text='Ю. Р. Вагин', tokens=[...], normal='Ю. Р. Вагин', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=28031, stop=28043, type='PER', text='Самосожжение', tokens=[...], normal='Самосожжение')\n",
      "DocSpan(start=28684, stop=28694, type='PER', text='Отравление', tokens=[...], normal='Отравление')\n",
      "DocSpan(start=29692, stop=29708, type='PER', text='Марине Цветаевой', tokens=[...], normal='Марина Цветаева', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=30317, stop=30326, type='PER', text='Джон Донн', tokens=[...], normal='Джон Донн', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=30382, stop=30389, type='PER', text='Самсона', tokens=[...], normal='Самсон', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=30729, stop=30737, type='PER', text='Мухаммед', tokens=[...], normal='Мухаммед', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=30837, stop=30849, type='PER', text='Отравившийся', tokens=[...], normal='Отравившийся')\n",
      "DocSpan(start=31027, stop=31043, type='PER', text='Абу Дауд (X век)', tokens=[...], normal='Абу Дауд (X век)', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=31066, stop=31074, type='PER', text='Мухаммед', tokens=[...], normal='Мухаммед', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=32540, stop=32546, type='PER', text='Иш Таб', tokens=[...], normal='Иш Таб', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=34536, stop=34543, type='PER', text='Петра I', tokens=[...], normal='Петр I', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=38334, stop=38340, type='PER', text='Пинель', tokens=[...], normal='Пинель', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=38343, stop=38351, type='PER', text='Эскироль', tokens=[...], normal='Эскироль', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=39045, stop=39054, type='PER', text='Томас Сас', tokens=[...], normal='Томас Сас', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=39468, stop=39485, type='PER', text='М. П. Мусоргского', tokens=[...], normal='М. П. Мусоргский', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=41295, stop=41309, type='PER', text='Бориса Положия', tokens=[...], normal='Борис Положия', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=41830, stop=41843, type='PER', text='Павел Астахов', tokens=[...], normal='Павел Астахов', fact=DocFact(slots=[...]))\n",
      "DocSpan(start=43888, stop=43890, type='PER', text='См', tokens=[...], normal='См')\n"
     ]
    }
   ],
   "source": [
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "\n",
    "# for span in doc.spans:\n",
    "#     if span.type == PER:\n",
    "#         print(span)\n",
    "\n",
    "doc.spans[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Легко увидеть, что...\n",
    "В общем и целом мы очень хорошо справляемся с местами локаций. Шутка про то, что самоубийство -- это имя, вообще разрыв. Также прикольно, что у нас \"Аффективные\" -- это тоже персона, а СИЗО -- это организация... Но происходит это по понятным причинам, а, значит, можно предугадать такое поведение и просто хорошенько предобработать текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Правило 1\n",
    "Далее первое правило нацелено на то, чтобы выделить в тексте какие-то заболевания, сопутствующие теме суицидологии. Я рассчитываю на то, что это позволит получить список \"симптоматик\" для более строгого надзора за людьми в группах риска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['серьёзных', 'заболеваниях']\n",
      "['душевную', 'боль']\n",
      "['болезнь', 'близкого']\n",
      "['соматическое', 'заболевание']\n",
      "['психическими', 'травмами']\n",
      "['хроническими', 'болями']\n",
      "['хронические', 'заболевания']\n",
      "['Соматические', 'болезни']\n",
      "['соматическими', 'болезнями']\n",
      "['хроническими', 'заболеваниями']\n",
      "['и', 'травмы']\n",
      "['онкологические', 'заболевания']\n",
      "['непереносимые', 'боли']\n",
      "['нефрологические', 'болезни']\n",
      "['инфекцией', 'повышенный']\n",
      "['соматических', 'болезней']\n",
      "['хронические', 'заболевания']\n",
      "['такие', 'заболевания']\n",
      "['язвенная', 'болезнь']\n",
      "['соматических', 'заболеваниях']\n",
      "['психических', 'заболеваний']\n",
      "['опасными', 'заболеваниями']\n",
      "['душевной', 'боли']\n",
      "['неизлечимые', 'болезни']\n",
      "['серьёзного', 'заболевания']\n",
      "['лечения', 'заболевания']\n",
      "['неизлечимому', 'заболеванию']\n",
      "['его', 'болезни']\n"
     ]
    }
   ],
   "source": [
    "from yargy import Parser, rule, and_\n",
    "from yargy.predicates import gram, is_capitalized, dictionary\n",
    "\n",
    "DESIESE = morph_pipeline([\n",
    "    'инфекция',\n",
    "    'болезнь',\n",
    "    'боль',\n",
    "    'заболевание',\n",
    "    'травма'\n",
    "])\n",
    "\n",
    "DESIESE_NAME = or_(\n",
    "    gram(\"NOUN\"),\n",
    "    gram(\"ADJF\"),\n",
    "    # gram(\"ADJS\")\n",
    ")\n",
    "\n",
    "RULE = or_(\n",
    "    rule(DESIESE, DESIESE_NAME),\n",
    "    rule(DESIESE_NAME, DESIESE),\n",
    ")\n",
    "\n",
    "parser = Parser(RULE)\n",
    "for match in parser.findall(text):\n",
    "    print([_.value for _ in match.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат:\n",
    "\n",
    "Получилось не очень хорошо, но и не очень плохо. Мы не смогли найти ВИЧ-инфекцию и бронхиальную астму, однако нашли большой перечень хороших \"звоночков\". Но также имеются и проблемы: нашли сочетания по типу \"его болезни\" и \"и травмы\". С одной стороны, это ок, а с другой -- до полной автоматизации тут далеко."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Правило 2\n",
    "Сделаем вид, что нам очень лень самим читать текст и мы хотим узнать, кто что сделал во сколько раз чаще, чтобы привести эти в дипломчике. Такие сравнения, благо, часто носят одну и ту же форму, поэтому мы без проблем можем это попробовать сделать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yargy import Parser, rule, or_\n",
    "from yargy.predicates import gram, is_capitalized, dictionary\n",
    "\n",
    "# в\n",
    "PREP = gram(\"PREP\")\n",
    "\n",
    "# 4\n",
    "NUMB = or_(\n",
    "    gram(\"NUMB\"),\n",
    "    gram(\"NUMR\")\n",
    ")\n",
    "\n",
    "# раза\n",
    "TIMES = morph_pipeline([\n",
    "    'раза',\n",
    "    'раз'\n",
    "])\n",
    "\n",
    "# чаще, реже\n",
    "ADJ = or_(\n",
    "    gram(\"ADJF\"),\n",
    "    gram(\"ADJS\"),\n",
    "    gram(\"COMP\")\n",
    ")\n",
    "\n",
    "# умирают, убивают, режут, кушают\n",
    "VERB = gram(\"VERB\")\n",
    "\n",
    "RULE = or_(\n",
    "    rule(PREP, NUMB, TIMES, ADJ, VERB), # \"в 10 раз чаще умирают\"\n",
    "    rule(PREP, NUMB, TIMES, ADJ, gram(\"NOUN\")), # в 10 раз чаще мужчин\n",
    "    rule(PREP, NUMB, TIMES), # \"в 10 раз\"\n",
    ")\n",
    "\n",
    "parser = Parser(RULE)\n",
    "for match in parser.findall(\"Мужчины совершают самоубийство в 4.34 раза чаще, чем женщины (хотя женщины совершают в 4 раза больше попыток самоубийства)\"):\n",
    "    print([_.value for _ in match.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И вот это уже незадача. Почему-то именно gram(\"NUMB\") никак не может распознать число 4 или 4.34. Почему? Для меня загадка и интернет на этот счет молчит.\n",
    "\n",
    "У pymorphy есть еще is_roman_number, но это нам как бы не подойдет... Очень странная и плохая дичь ._.\n",
    "\n",
    "P.S. -- ROMN тоже не работает ;-;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Правило 3\n",
    "\n",
    "Ну что ж, после той неудачи, давайте попробуем найти что-нибудь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', 'тысячи', 'человек']\n",
      "['1', 'миллиона', 'человек']\n",
      "['110', 'Уголовного', 'кодекса']\n",
      "['120', '-', 'ФЗ']\n",
      "['№', '139', '-', 'ФЗ']\n",
      "['124', 'УК', 'РФ']\n"
     ]
    }
   ],
   "source": [
    "from yargy import Parser, rule, or_, not_\n",
    "from yargy.predicates import gram, is_capitalized, dictionary, lte, gte, is_single\n",
    "\n",
    "\n",
    "# Номер\n",
    "NUMB_FIR = morph_pipeline([\n",
    "    \"номер\",\n",
    "    \"#\",\n",
    "    \"№\"\n",
    "])\n",
    "\n",
    "# Числительное (а вот так работает))))\n",
    "NUMB_SEC = and_(\n",
    "    gte(1),\n",
    "    lte(999)\n",
    ")\n",
    "\n",
    "# УК, РФ, ФЗ и прочее\n",
    "LAW = and_(\n",
    "    gram(\"NOUN\"),\n",
    "    is_single(),\n",
    "    not_(gram(\"PREP\")), # ну да, предлоги же -- это существительные...\n",
    "    not_(gram(\"COMP\")),\n",
    "    not_(gram(\"CONJ\")),\n",
    "    not_(gram(\"NUMR\")),\n",
    "    or_(\n",
    "        gram(\"nomn\"),\n",
    "        gram(\"accs\"),\n",
    "        gram(\"gent\")\n",
    "    )\n",
    ")\n",
    "\n",
    "RULE = or_(\n",
    "    rule(NUMB_FIR, NUMB_SEC, \"-\", LAW), # № 100-ФЗ\n",
    "    rule(NUMB_FIR, NUMB_SEC, LAW), # № 120 УК\n",
    "    rule(NUMB_SEC, \"-\", LAW), # 120-ФЗ\n",
    "    rule(NUMB_SEC, LAW, LAW), # 120 УК РФ, 120 ГК РФ\n",
    ")\n",
    "\n",
    "parser = Parser(RULE)\n",
    "for match in parser.findall(text):\n",
    "    print([_.value for _ in match.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат:\n",
    "было бы проще и лучше воспользоваться регуляркой со словарем \"УК\", \"ФЗ\", \"ГК\", однако так как тут природа текста не очень профессиональная, она позволила также определить и \"Уголовного кодекса\", что круто, даже несмотря на то, что мне так и не удалось отрезать эти несчастные \"3 тысячи человек\" и \"1 миллиона человек\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
