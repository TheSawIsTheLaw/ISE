{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2 -- \"Читаем хабр, используем Наташу, изучаем именованные сущности)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.\n",
    "\n",
    "Открываем википедию *(за что мне это)*, выбираем статью не менее 3000 слов, определяем именованные сущности, которые фигурируют в тексте, определяем, какие границы этих сущностей хотим задать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Хорошо, что так можно сделать\n",
    "# Жаль, что мне это не понравилось\n",
    "\n",
    "response = requests.get(\n",
    "    'https://ru.wikipedia.org/w/api.php',\n",
    "    params={\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "        'titles': 'Самоубийство',\n",
    "    }\n",
    ").json()\n",
    "\n",
    "f = open('analyze.txt', 'w')\n",
    "f.write(next(iter(response['query']['pages'].values()))['extract'])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ┌► Чрезвычайно  advmod\n",
      "        ┌►└─ сложный      amod\n",
      "        │ ┌► этический    amod\n",
      "        └─└─ вопрос       nsubj\n",
      "┌─┌─────└─┌─ представляет \n",
      "│ │     ┌─└► отнесение    obj\n",
      "│ │     │ ┌► к            case\n",
      "│ │ ┌───└►└─ самоубийству nmod\n",
      "│ │ │ ┌────► (            punct\n",
      "│ │ │ │ ┌──► либо         cc\n",
      "│ │ │ │ │ ┌► к            case\n",
      "│ │ └►└─└─└─ убийству     parataxis\n",
      "│ │     └──► )            punct\n",
      "│ └────────► эвтаназии    obl\n",
      "└──────────► .            punct\n"
     ]
    }
   ],
   "source": [
    "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, PER, NamesExtractor, Doc \n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "text = open('analyze.txt').read()\n",
    "\n",
    "doc = Doc(text)\n",
    "\n",
    "# Рубрика \"угадай, какую последовательность вызовов надо использовать, чтобы все это заработало\"\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "for it in doc.tokens:\n",
    "    it.lemmatize(morph_vocab)\n",
    "\n",
    "doc.parse_syntax(syntax_parser)\n",
    "doc.sents[1].syntax.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tag_ner(ner_tagger)\n",
    "doc.ner.print()\n",
    "# аккуратно, тут будет много буков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSpan(stop=13, type='PER', text='Самоуби́йство', tokens=[...], normal='Самоуби́йство'),\n",
       " DocSpan(start=471, stop=477, type='LOC', text='Европы', tokens=[...], normal='Европа'),\n",
       " DocSpan(start=583, stop=626, type='ORG', text='Всемирной организации здравоохранения (ВОЗ)', tokens=[...], normal='Всемирная организация здравоохранения (ВОЗ)'),\n",
       " DocSpan(start=663, stop=668, type='LOC', text='Земли', tokens=[...], normal='Земля'),\n",
       " DocSpan(start=746, stop=749, type='ORG', text='ВОЗ', tokens=[...], normal='ВОЗ'),\n",
       " DocSpan(start=1030, stop=1047, type='LOC', text='Латинской Америке', tokens=[...], normal='Латинская Америка'),\n",
       " DocSpan(start=1119, stop=1130, type='LOC', text='Центральной', tokens=[...], normal='Центральная'),\n",
       " DocSpan(start=1133, stop=1148, type='LOC', text='Северной Европе', tokens=[...], normal='Северная Европа'),\n",
       " DocSpan(start=1150, stop=1166, type='LOC', text='Северной Америке', tokens=[...], normal='Северная Америка'),\n",
       " DocSpan(start=1168, stop=1186, type='LOC', text='Юго-Восточной Азии', tokens=[...], normal='Юго-Восточная Азия'),\n",
       " DocSpan(start=1189, stop=1203, type='LOC', text='Западной части', tokens=[...], normal='Западная часть'),\n",
       " DocSpan(start=1204, stop=1217, type='LOC', text='Тихого океана', tokens=[...], normal='Тихий океан'),\n",
       " DocSpan(start=1219, stop=1228, type='LOC', text='Австралия', tokens=[...], normal='Австралия'),\n",
       " DocSpan(start=1230, stop=1236, type='LOC', text='Канада', tokens=[...], normal='Канада'),\n",
       " DocSpan(start=1238, stop=1243, type='LOC', text='Индия', tokens=[...], normal='Индия')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "\n",
    "for span in doc.spans:\n",
    "    if span.type == PER:\n",
    "        span.extract_fact(names_extractor)\n",
    "\n",
    "doc.spans[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(start=583, stop=592, text='Всемирной', id='5_3', head_id='5_4', rel='amod', pos='ADJ', feats=<Gen,Pos,Fem,Sing>, lemma='всемирный'),\n",
       " DocToken(start=593, stop=604, text='организации', id='5_4', head_id='5_2', rel='nmod', pos='NOUN', feats=<Inan,Gen,Fem,Sing>, lemma='организация'),\n",
       " DocToken(start=605, stop=620, text='здравоохранения', id='5_5', head_id='5_4', rel='nmod', pos='NOUN', feats=<Inan,Gen,Neut,Sing>, lemma='здравоохранение'),\n",
       " DocToken(start=621, stop=622, text='(', id='5_6', head_id='5_7', rel='punct', pos='PUNCT', lemma='('),\n",
       " DocToken(start=622, stop=625, text='ВОЗ', id='5_7', head_id='5_5', rel='parataxis', pos='PROPN', feats=<Inan,Nom,Fem,Sing>, lemma='воз'),\n",
       " DocToken(start=625, stop=626, text=')', id='5_8', head_id='5_7', rel='punct', pos='PUNCT', lemma=')')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.spans[2].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
